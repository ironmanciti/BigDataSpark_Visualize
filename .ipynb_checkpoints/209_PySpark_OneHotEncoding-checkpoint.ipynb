{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e37546f",
   "metadata": {},
   "source": [
    "# One-hot encoding in PySpark\n",
    "\n",
    "PySpark에서 원-핫 인코딩을 수행하려면 다음을 수행해야 합니다.\n",
    "1. StringIndexer를 사용하여 범주 열을 숫자 열(0, 1, ...)로 변환\n",
    "2. OneHotEncoder를 사용하여 숫자 열을 원-핫 인코딩 열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773afb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066fc5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/13 15:50:39 WARN Utils: Your hostname, oyeongje-ui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 172.30.1.24 instead (on interface en0)\n",
      "22/12/13 15:50:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/13 15:50:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/13 15:50:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/13 15:50:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.30.1.24:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe86d456770>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrame\").config(\"spark.sql.repl.eagerEval.enabled\", True).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa79719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| name|class|\n",
      "+-----+-----+\n",
      "| Alex|    B|\n",
      "|  Bob|    A|\n",
      "|Cathy|    B|\n",
      "| Dave|    C|\n",
      "| Eric|    D|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [['Alex','B'], ['Bob','A'], ['Cathy','B'], ['Dave','C'], ['Eric','D']]\n",
    "df = spark.createDataFrame(rows, ['name', 'class'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83927c74",
   "metadata": {},
   "source": [
    "### 첫 번째 단계 - StringIndexer를 사용하여 클래스 열을 숫자 열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a477a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|class|class_|\n",
      "+-----+-----+------+\n",
      "| Alex|    B|   0.0|\n",
      "|  Bob|    A|   1.0|\n",
      "|Cathy|    B|   0.0|\n",
      "| Dave|    C|   2.0|\n",
      "| Eric|    D|   3.0|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='class', outputCol='class_').fit(df)\n",
    "df_r = indexer.transform(df)\n",
    "df_r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0c0d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'A', 'C', 'D']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187d207",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "희소 벡터는 다음 세 가지 값으로 정의됩니다.\n",
    "\n",
    "size: 벡터의 크기(카테고리 수에서 1을 뺀 값)  \n",
    "인덱스: 값을 보유하는 벡터의 인덱스  \n",
    "값: 인덱스에 있는 값  \n",
    "\n",
    "벡터 (3,[0],[1.0])를 예로 들어 보겠습니다. 하나의 범주가 기본 범주로 사용되기 때문에 4개의 고유한 범주(A,B,C,D)가 있지만 벡터의 크기는 3입니다.  \n",
    "중간 값 [0]과 세 번째 값 [1.0]은 벡터에서 인덱스 위치 0을 1.0으로 채워야 함을 의미합니다. 희소 벡터의 다른 모든 값은 0으로 채워집니다.\n",
    "\n",
    "원-핫 인코딩된 벡터(3,[],[])의 두 번째 및 세 번째 값은 모두 비어 있습니다. 이것은 벡터가 0으로 채워져 있음을 의미합니다. 즉, 범주 D가 기본 범주로 취급됩니다. 이것이 크기 3의 벡터로 4개의 고유 범주를 나타낼 수 있는 이유입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617e7384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-------------+\n",
      "| name|class|class_|    class_ohe|\n",
      "+-----+-----+------+-------------+\n",
      "| Alex|    B|   0.0|(3,[0],[1.0])|\n",
      "|  Bob|    A|   1.0|(3,[1],[1.0])|\n",
      "|Cathy|    B|   0.0|(3,[0],[1.0])|\n",
      "| Dave|    C|   2.0|(3,[2],[1.0])|\n",
      "| Eric|    D|   3.0|    (3,[],[])|\n",
      "+-----+-----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['class_'], outputCols=['class_ohe']).fit(df_r)\n",
    "df_onehot = encoder.transform(df_r)\n",
    "df_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9248c0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alex', class='B', class_=0.0, class_ohe=SparseVector(3, {0: 1.0})),\n",
       " Row(name='Bob', class='A', class_=1.0, class_ohe=SparseVector(3, {1: 1.0})),\n",
       " Row(name='Cathy', class='B', class_=0.0, class_ohe=SparseVector(3, {0: 1.0})),\n",
       " Row(name='Dave', class='C', class_=2.0, class_ohe=SparseVector(3, {2: 1.0})),\n",
       " Row(name='Eric', class='D', class_=3.0, class_ohe=SparseVector(3, {}))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onehot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c1e0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- class_: double (nullable = false)\n",
      " |-- class_ohe: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_onehot.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64abcc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-------------+-------------+\n",
      "| name|class|class_|    class_ohe|     features|\n",
      "+-----+-----+------+-------------+-------------+\n",
      "| Alex|    B|   0.0|(3,[0],[1.0])|[1.0,0.0,0.0]|\n",
      "|  Bob|    A|   1.0|(3,[1],[1.0])|[0.0,1.0,0.0]|\n",
      "|Cathy|    B|   0.0|(3,[0],[1.0])|[1.0,0.0,0.0]|\n",
      "| Dave|    C|   2.0|(3,[2],[1.0])|[0.0,0.0,1.0]|\n",
      "| Eric|    D|   3.0|    (3,[],[])|    (3,[],[])|\n",
      "+-----+-----+------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['class_ohe'], outputCol='features')\n",
    "\n",
    "output = assembler.transform(df_onehot)\n",
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d62279ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 0.0, 0.0])),\n",
       " Row(features=DenseVector([0.0, 1.0, 0.0])),\n",
       " Row(features=DenseVector([1.0, 0.0, 0.0])),\n",
       " Row(features=DenseVector([0.0, 0.0, 1.0])),\n",
       " Row(features=SparseVector(3, {}))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.select(\"features\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01640d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
